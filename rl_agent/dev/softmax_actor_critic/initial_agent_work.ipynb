{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f938c4bc-21e7-4c41-91ba-1298c6e6ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "837ddc77-1658-48a7-bc1d-f92e1e38799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eff225-628b-46fe-9ca1-26effd517cc8",
   "metadata": {},
   "source": [
    "### General Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a0205-596c-42c0-a3b6-8afabe1cd3f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b46c3b-db28-4915-ae4d-97ad707df240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prep_data(data_path):\n",
    "    raw_df = pd.read_csv(data_path)\n",
    "    data_df = raw_df.copy()\n",
    "    # Make current positive\n",
    "    data_df = data_df.drop_duplicates(subset=['motor_1_position','motor_2_position'])\n",
    "    data_df['I_ivp_1'] = data_df['I_ivp_1'].abs()\n",
    "    data_df['power'] = data_df['I_ivp_1'] * data_df['V_ivp_1']\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "949a872e-633f-4d7f-a911-8fa0ab392a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_motor_positions_to_index(position_tuple):\n",
    "    # position tuple is (m1 position, m2 position)\n",
    "    return (int(position_tuple[0]//5), int(position_tuple[1]//5))\n",
    "\n",
    "def convert_index_to_motor_positions(index_tuple):\n",
    "    return (index_tuple[0]*5, index_tuple[1]*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd95a6-c7cd-4289-a462-10056a20e5bf",
   "metadata": {},
   "source": [
    "#### Policy helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1839ace-528a-400e-a5c7-cbc2cc4d16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_prob(actor_av_array, temperature=1):\n",
    "    # Divide all values by temperature\n",
    "    temperature_array = actor_av_array/temperature\n",
    "    \n",
    "    # Find max state value\n",
    "    max_value = np.max(actor_av_array)\n",
    "    \n",
    "    # Generate the numerator for each element by subtracting max value and exponentiating\n",
    "    numerator_array = actor_av_array - max_value\n",
    "    numerator_array = np.exp(numerator_array)\n",
    "\n",
    "    # Get the denominator by summing all values in the numerator\n",
    "    denominator_array = np.sum(numerator_array)\n",
    "    \n",
    "    \n",
    "    # Calculate the softmax value array and return to agent\n",
    "    softmax_array = numerator_array / denominator_array\n",
    "    \n",
    "    return softmax_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d30d9-816c-4b97-a8f7-6e1817d70857",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Experiment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c9354-8c79-49f3-8b0b-e0ac64cdc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value_error(agent_array, env_array):\n",
    "    return np.sqrt((agent_array - env_array)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b185c2-663c-4d9a-bc98-6d85460ffbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_dict_to_df(progress_dict):\n",
    "    dict_list = []\n",
    "    for x in progress_dict.keys():\n",
    "        temp_dict = progress_dict[x]\n",
    "        temp_dict['step'] = x\n",
    "        dict_list.append(temp_dict)\n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63427c-3527-4cd8-9051-195a85ebdc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_experiment(exp_agent, exp_env, steps, interval):\n",
    "    progress_dict = {}\n",
    "    exp_agent.agent_start()\n",
    "    progress_dict['0'] = {\n",
    "                'state_value': exp_agent.get_state_value_array(),\n",
    "                'rolling_power': exp_agent.get_agent_rolling_power(),\n",
    "                'mse': calculate_value_error(exp_agent.get_state_value_array(), exp_env.get_reward_array()),\n",
    "                'state_visits': exp_agent.get_state_visits(),\n",
    "                'total_energy': exp_agent.get_agent_energy()\n",
    "            }\n",
    "    for i in tqdm(range(1, steps + 1)):\n",
    "        exp_agent.agent_step()\n",
    "        if i%interval == 0:\n",
    "            progress_dict[str(i)] = {\n",
    "                'state_value': exp_agent.get_state_value_array(),\n",
    "                'rolling_power': exp_agent.get_agent_rolling_power(),\n",
    "                'mse': calculate_value_error(exp_agent.get_state_value_array(), exp_env.get_reward_array()),\n",
    "                'state_visits': exp_agent.get_state_visits(),\n",
    "                'total_energy': exp_agent.get_agent_energy()\n",
    "            }\n",
    "    progress_df = progress_dict_to_df(progress_dict)\n",
    "    return exp_agent, progress_dict, progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81dff5d-d828-495d-b50d-a9a3ae1a03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_power(progress_df, exp_env, height, width):\n",
    "    max_output = exp_env.get_reward_array().max()\n",
    "    progress_df['env_max'] = max_output\n",
    "    progress_df['optimal_energy'] = progress_df['step'].astype(float) * max_output\n",
    "    progress_df['difference'] = (progress_df['rolling_power'] - progress_df['env_max']) / progress_df['env_max']\n",
    "    make_subplots_plot(df=progress_df, x='step', subplot_group_list=[\n",
    "    {\n",
    "        'title': 'Reward Comparison (Agent vs Max)',\n",
    "         'columns': ['env_max','rolling_power']\n",
    "    },\n",
    "        {\n",
    "        'title': 'Energy Comparison (Agent vs Max)',\n",
    "         'columns': ['total_energy', 'optimal_energy']\n",
    "        }\n",
    "    ], height=height, width=width\n",
    "                 )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8327a61-ebab-4215-b354-77151b048ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subplots_plot(df, x, subplot_group_list, height=400, width=400, plot_title=''):\n",
    "    fig = make_subplots(rows=len(subplot_group_list), cols=1,\n",
    "                       subplot_titles=[x['title'] for x in subplot_group_list])\n",
    "    \n",
    "    for i in range(len(subplot_group_list)):\n",
    "        row = i+1\n",
    "        title = subplot_group_list[i]['title']\n",
    "        for column_name in subplot_group_list[i]['columns']:\n",
    "            fig.append_trace(go.Scatter(\n",
    "                x=df[x],\n",
    "                y=df[column_name], name=column_name\n",
    "            ), row=row, col=1)\n",
    "\n",
    "    fig.update_layout(height=height, width=width, title_text=plot_title)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0ed7f-fca1-4a86-90da-c2d4a8b76fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_array_evolution(exp_progress_dict, exp_interval, field, width_plot, height_plot, zmax=None, zmin=None):\n",
    "    matrix_list = [exp_progress_dict[x][field] for x in exp_progress_dict.keys()]\n",
    "    fig = go.Figure(\n",
    "        data=[go.Heatmap(z=matrix_list[0], zmax=zmax, zmin=zmin)],layout=go.Layout(\n",
    "                title=\"Step 0\",\n",
    "                updatemenus=[dict(\n",
    "                    type=\"buttons\",\n",
    "                    buttons=[dict(label=\"Play\",\n",
    "                                  method=\"animate\",\n",
    "                                  args=[None]),\n",
    "                            dict(label=\"Pause\",\n",
    "                                 method=\"animate\",\n",
    "                                 args=[None,\n",
    "                                       {\"frame\": {\"duration\": 0, \"redraw\": False},\n",
    "                                        \"mode\": \"immediate\",\n",
    "                                        \"transition\": {\"duration\": 0}}],\n",
    "                                 )])]\n",
    "            ),\n",
    "            frames=[go.Frame(data=[go.Heatmap(z=matrix_list[i])], \n",
    "                             layout=go.Layout(title_text=f\"Step {i * exp_interval}\")) for i in range(1, len(matrix_list))]\n",
    "    )\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    fig.update_layout({\n",
    "        'height': height_plot,\n",
    "        'width': width_plot}\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748f703-5499-4dcb-8843-f213e728d516",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769522c-bc81-4f81-9618-d8337978124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarEnv:\n",
    "    def __init__(self, reward_data_path, shape=(37,37), control=None):\n",
    "        self.shape = shape\n",
    "        self.reward_array = np.zeros(shape)\n",
    "        # load in reward data\n",
    "        rewards = load_and_prep_data(reward_data_path)\n",
    "        for index, row in rewards.iterrows():\n",
    "            motor_1_index = int(row['motor_1_position'].item()//5)\n",
    "            motor_2_index = int(row['motor_2_position'].item()//5)\n",
    "            position_reward = row['power'].item()\n",
    "            self.reward_array[motor_1_index][motor_2_index] = position_reward\n",
    "        if control is not None:\n",
    "            self.reward_array = np.full(shape, control)\n",
    "    \n",
    "    # For debugging\n",
    "    def get_reward_array(self):\n",
    "        return self.reward_array\n",
    "    \n",
    "    def get_env_shape(self):\n",
    "        return self.reward_array.shape\n",
    "                                          \n",
    "    # Not needed right now\n",
    "    def env_init(self):\n",
    "        \"\"\"\n",
    "        Setup for the environment called when the experiment first starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    # Not needed right now\n",
    "    def env_start(self):\n",
    "        \"\"\"\n",
    "        The first method called when the experiment starts, called before the\n",
    "        agent starts.\n",
    "\n",
    "        Returns:\n",
    "            The first state from the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def env_step(self, action):\n",
    "        \"\"\"A step taken by the environment.\n",
    "\n",
    "        Args:\n",
    "            action: The action taken by the agent, a tuple of motor positions\n",
    "\n",
    "        Returns:\n",
    "            (float, state): a tuple of the reward, state\n",
    "        \"\"\"\n",
    "        index_tuples = convert_motor_positions_to_index(action)\n",
    "        return self.reward_array[index_tuples[0]][index_tuples[1]], convert_index_to_motor_positions(index_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac738823-8ec9-4565-b25a-cca2f587288d",
   "metadata": {},
   "source": [
    "Visualizing the reward array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec059f9e-08a3-4211-8617-0a2285fc9384",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Agent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680ef40-22fa-445b-ad71-31b7c2a07567",
   "metadata": {},
   "source": [
    "Agent will start out as a TD(0) agent with epsilon-greedy policy\n",
    "* V(st) = V(st) + step_size * [Rewardt+1 + discount_factor * V(st+1) - V(st)]\n",
    "\n",
    "**Improvements**\n",
    "The new agent will use TD(0) but has some slight changes to account for the visitation bias in the last experiment.\n",
    "\n",
    "v(st) <- v(st) + step_size * [reward - v(st)] * [1 + tanh(abs((reward - v(st))/v(st)))]^chi\n",
    "\n",
    "Here chi is a tuneable parameter but will start as a factor of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0845b6-0ab8-45b4-9370-b28a809ce3fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "#### Quick experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268d53b-2701-426e-96bc-b477102c291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_value = 4.32\n",
    "chi = 2\n",
    "reward = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d144a-19de-4f9f-b1a8-8c211689df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_value + 0.1*(reward - last_value) * (1 + np.tanh(np.abs((reward - last_value)/last_value)))**chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d6701-7083-412c-86c4-22e30df7349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_thing_tanh(initial_value, chi, reward, steps):\n",
    "    last_value = initial_value\n",
    "    for i in range(steps):\n",
    "        update = last_value + 0.1*(reward - last_value) * (1 + np.tanh(np.abs((reward - last_value)/last_value)))**chi\n",
    "        last_value = update\n",
    "        print(round(update,4))\n",
    "        \n",
    "def simulate_thing_no_scaling(initial_value, chi, reward, steps):\n",
    "    last_value = initial_value\n",
    "    for i in range(steps):\n",
    "        update = last_value + 0.1*(reward - last_value)\n",
    "        last_value = update\n",
    "        print(round(update,4))\n",
    "        \n",
    "def simulate_thing_notanh(initial_value, chi, reward, steps):\n",
    "    last_value = initial_value\n",
    "    for i in range(steps):\n",
    "        update = last_value + 0.1*(reward - last_value) * (1 + np.abs((reward - last_value)/last_value))**chi\n",
    "        last_value = update\n",
    "        print(round(update,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745117a-7617-4f39-b365-57973cb2e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_thing_no_scaling(0.0001, 2, 0.07, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f0561-d1c0-43e7-b7cf-55343ac2c28d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376b5ee-2468-4fd9-9efd-27976e6e5702",
   "metadata": {},
   "source": [
    "#### Agent functions / class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710ccbe-0f6a-4d6b-9497-5455942a158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_avg_calc(value, last_value, window):\n",
    "    return (1/window)*value + (1-(1/window))*last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469e872-b860-43ca-858f-6c185cb07c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SolarAgent:\n",
    "    def __init__(self, step_size, epsilon, chi, initialization_value, env, rolling_window):\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        self.chi = chi\n",
    "        self.env_shape = env.get_env_shape()\n",
    "        self.state_values = np.full(self.env_shape, initialization_value)\n",
    "        self.state_visits = np.zeros(self.env_shape)\n",
    "        self.last_state = None\n",
    "        self.state = None\n",
    "        self.last_reward = None\n",
    "        self.reward = None\n",
    "        self.env = env\n",
    "        self.total_energy = 0\n",
    "        self.rolling_power = 0\n",
    "        self.rolling_window = rolling_window\n",
    "        self.transition_dict = None\n",
    "        random.seed(RANDOM_SEED)\n",
    "    \n",
    "    def get_state_value_array(self):\n",
    "        return self.state_values.copy()\n",
    "    \n",
    "    def agent_policy(self):\n",
    "        # if random greedy\n",
    "#         random.seed(RANDOM_SEED)\n",
    "        if random.random() <= self.epsilon:\n",
    "            action = random_array_index(self.state_values)\n",
    "        # otherwise arg max\n",
    "        else:\n",
    "            action = arg_max_array_index(self.state_values)\n",
    "        return convert_index_to_motor_positions(action)\n",
    "    \n",
    "    def agent_start(self):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            self.last_action [int] : The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.last_state = [90,90]\n",
    "        self.last_reward = 0\n",
    "        self.state, self.reward = self.env.env_step(self.last_state)\n",
    "    \n",
    "    def get_state_values(self, state):\n",
    "        converted_index = convert_motor_positions_to_index(state)\n",
    "        return self.state_values[converted_index[0]][converted_index[1]]\n",
    "    \n",
    "    def agent_step(self):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward [float]: the reward received for taking the last action taken\n",
    "            state [int]: the state from the environment's step, where the agent ended up after the last step\n",
    "        Returns:\n",
    "            self.last_action [int] : The action the agent is taking.\n",
    "        \"\"\"\n",
    "        # Make a policy decision\n",
    "        action = self.agent_policy()\n",
    "        \n",
    "        # Interact with the environment\n",
    "        reward, next_state = self.env.env_step(action)\n",
    "        \n",
    "        # TD Update\n",
    "        last_state_value = self.get_state_values(self.last_state)\n",
    "        new_state_value = self.get_state_values(next_state)\n",
    "        error_term = reward - last_state_value\n",
    "        scaling_term = (1 + np.tanh(np.abs(error_term/last_state_value)))**self.chi\n",
    "        last_state_index = convert_motor_positions_to_index(self.last_state)\n",
    "        self.state_values[last_state_index[0]][last_state_index[1]] = last_state_value + self.step_size * error_term * scaling_term\n",
    "        \n",
    "        # For debugging\n",
    "        self.transition_dict = {\n",
    "            'reward': reward,\n",
    "            'new_state_value': new_state_value,\n",
    "            'last_state_value': last_state_value,\n",
    "            'error_term': error_term,\n",
    "            'updated_value_estimate': last_state_value + self.step_size * error_term\n",
    "        }\n",
    "        \n",
    "        # Update internal variables\n",
    "        self.last_state = next_state\n",
    "        \n",
    "        # For tracking\n",
    "        self.total_energy += reward\n",
    "        self.rolling_power = rolling_avg_calc(reward, self.rolling_power, self.rolling_window)\n",
    "        self.state_visits[last_state_index[0]][last_state_index[1]] += 1\n",
    "    \n",
    "    def get_agent_energy(self):\n",
    "        return self.total_energy\n",
    "    \n",
    "    def get_agent_rolling_power(self):\n",
    "        return self.rolling_power\n",
    "    \n",
    "    def get_transition_dict(self):\n",
    "        return self.transition_dict\n",
    "    \n",
    "    def get_state_visits(self):\n",
    "        return self.state_visits.copy()\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46258b2-b13c-41dd-9e5a-5ca28d90076c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Resume here with experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357ddf9-2eb9-4c27-aa7e-856c90a0544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting max/min\n",
    "zmax_plot = 0.015\n",
    "zmin_plot = 0\n",
    "width_plot = 400\n",
    "height_plot = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659ce2e-da1b-42a0-84d0-1ae7e6edf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment factors\n",
    "exp_env_control = None\n",
    "exp_data_path = '../../../rl_agent/simulation_data/data/corrected_motors/run_5_kitchen_no_lights.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59270191-0081-447e-b68c-80c6e3d5f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set runtime values\n",
    "steps = 100000\n",
    "interval = 1000\n",
    "\n",
    "# Agent factors\n",
    "exp_step_size = 0.1\n",
    "exp_epsilon = 0.05\n",
    "exp_chi = 1.1\n",
    "exp_initialization_value = 0.02\n",
    "rolling_avg_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a67299-2264-4a6e-bab9-1fc64e0da7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_env = SolarEnv(reward_data_path=exp_data_path, shape=(37,37), control=exp_env_control)\n",
    "experiment_agent = SolarAgent(step_size=exp_step_size, epsilon=exp_epsilon, \n",
    "                              chi=exp_chi, initialization_value=exp_initialization_value, \n",
    "                              env=experiment_env, rolling_window=rolling_avg_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf2922-97d1-475b-8339-96c152178b4c",
   "metadata": {},
   "source": [
    "#### Initiate an experiment\n",
    "Remember to re-initialize the agent above or else it will resume learning on the exisitng agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56169d-6131-46c9-b9b3-a790b79bd1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "experiment_agent, progress_dict, progress_df = run_agent_experiment(exp_agent=experiment_agent, \n",
    "                                                                   exp_env=experiment_env, steps=steps, interval=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7d3ee-fae0-46bb-9cae-858cc6f914fb",
   "metadata": {},
   "source": [
    "#### Show the agent difference from known optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e089d-ba49-432f-a365-8032603f1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_power(progress_df, experiment_env, height=600, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0d1857-2d03-4bb9-adf8-6e9b43e14862",
   "metadata": {},
   "source": [
    "#### Visualize state visits and learned values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ea419-aa62-4162-9975-ac1f318c2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_max_step = True\n",
    "specific_step = '100000'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8150cc-903e-4eba-bd7e-6d86af1c8312",
   "metadata": {},
   "source": [
    "State visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca816f-012b-4e4c-8c7f-4f0b0417b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(progress_dict[(specific_step if not do_max_step else str(steps))]['state_visits'], width=width_plot, height=height_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ff4a1-fa21-475c-86c1-0ca6c6b971e9",
   "metadata": {},
   "source": [
    "State values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0630444-7a92-405c-9acc-6d8dab7e2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(progress_dict[(specific_step if not do_max_step else str(steps))]['state_value'], width=width_plot, \n",
    "          height=height_plot, zmin=zmin_plot, zmax=zmax_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0945d39-f673-4897-97c2-aa2fdb125d2a",
   "metadata": {},
   "source": [
    "#### Environment True Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c2d0f-778b-4b03-9983-0d22fadb977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(experiment_env.get_reward_array(), width=width_plot, height=height_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4eaa99-d91e-4ca2-a5cc-1c0e5470661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array_evolution(progress_dict, interval, 'state_visits', width_plot, height_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ed37e-28a0-472b-a036-714c7dc1a198",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visulaize evolution of learned state value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6398b-6be2-4f06-839c-d426b342c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array_evolution(progress_dict, interval, 'state_value', width_plot, height_plot, zmax_plot, zmin_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3221bf-843c-4766-b827-1682a3bf7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE vs steps\n",
    "px.scatter(progress_df, x='step', y='rolling_power', width=width_plot, height=height_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72827c36-139e-4032-8eab-65d772cb6d04",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff4000-d540-4893-a785-11236d03159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request codes\n",
    "MOTOR_CONTROL = 1000\n",
    "STATE_REQUEST = 2000\n",
    "RESET_CODE = 6666\n",
    "\n",
    "def scan_space(arduino):\n",
    "    # Run start\n",
    "    run_start = time.time()\n",
    "    data_dict_list = []\n",
    "    last_motor_interval = 0\n",
    "    last_measure_interval = -1\n",
    "    motor_frequency = 2\n",
    "    measure_frequency = 1\n",
    "    # Set timeouts\n",
    "    abort = False\n",
    "    \n",
    "    for xy_degree in range(0, 181, 5):\n",
    "        for yz_degree in range(0, 181, 5):\n",
    "            si.write_serial_line(arduino, [MOTOR_CONTROL, xy_degree, yz_degree], print_message=False)\n",
    "            new_message, abort = si.listen_for_serial(arduino)\n",
    "            if new_message is not None and not abort:\n",
    "                data_dict_list.append(new_message)\n",
    "            elif abort:\n",
    "                break\n",
    "            else:\n",
    "                print('Empty message received without abort issue')\n",
    "            time.sleep(0.1) # Wait for steady state\n",
    "        if abort:\n",
    "            break\n",
    "        print('xy:',xy_degree,'yz:',yz_degree)\n",
    "    # Write back to start state\n",
    "    write_serial_line(arduino, [si.MOTOR_CONTROL, 90, 90])\n",
    "\n",
    "    return pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07994661-9e43-4372-b543-b2641c5cad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('\\nARDUINO CONTROL TESTING')\n",
    "    print('-------------------------')\n",
    "    # Initialize serial port\n",
    "    print('\\nIniitalizing device...')\n",
    "    serial_port = '/dev/cu.usbmodem14101'\n",
    "    baud_rate = 9600\n",
    "    timeout = 5\n",
    "    arduino = si.initialize_serial(serial_port=serial_port, baud_rate=baud_rate, timeout=timeout)\n",
    "    print('\\t - SUCCESS: Device initialized.')\n",
    "    \n",
    "    si.write_serial_line(arduino, [MOTOR_CONTROL, 90, 90])\n",
    "\n",
    "    # Run a loop where motor position incremented every 5 seconds, print out message\n",
    "    print('\\nBeginning loop sequence...')\n",
    "#     data = scan_space(arduino)\n",
    "    print('\\t - Loop complete.')\n",
    "\n",
    "    # Add relative time to returned data and print out\n",
    "#     data['t_relative'] = data['timestamp'] - data['timestamp'].iloc[0]\n",
    "    print('\\nData broadcasted by Arduino:\\n')\n",
    "    \n",
    "#     data.to_csv('/Users/jackogrady/Git/rl-solar/rl_agent/simulation_data/data/run_6_kitchen_no_lights_swapped_motors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55694cd-2430-4233-943d-51758fc16cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1b648-e1a8-4d7d-84ca-56d3c3b878eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_serial_line(arduino, [1000, 180, 90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e08c6-02c9-4216-80d9-4968f08b292b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
