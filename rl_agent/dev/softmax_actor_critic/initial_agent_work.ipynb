{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d6b28e-ab99-47fe-9e08-0fe832fb4677",
   "metadata": {},
   "source": [
    "# Softmax Actor Critic with Avg Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f57b0-3416-49f0-941e-564df626956c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f938c4bc-21e7-4c41-91ba-1298c6e6ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837ddc77-1658-48a7-bc1d-f92e1e38799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "ARRAY_DIMENSION_TUPLE = (37,37)\n",
    "MOTOR_ANGLE_POWER_DRAW = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eff225-628b-46fe-9ca1-26effd517cc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### General Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a0205-596c-42c0-a3b6-8afabe1cd3f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b46c3b-db28-4915-ae4d-97ad707df240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prep_data(data_path):\n",
    "    raw_df = pd.read_csv(data_path)\n",
    "    data_df = raw_df.copy()\n",
    "    # Make current positive\n",
    "    data_df = data_df.drop_duplicates(subset=['motor_1_position','motor_2_position'])\n",
    "    data_df['I_ivp_1'] = data_df['I_ivp_1'].abs()\n",
    "    data_df['power'] = data_df['I_ivp_1'] * data_df['V_ivp_1']\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b14ffe-987c-42e4-b03c-e1d48e0d935a",
   "metadata": {},
   "source": [
    "#### Index to motor position conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949a872e-633f-4d7f-a911-8fa0ab392a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_motor_positions_to_2d_index(position_tuple):\n",
    "    # position tuple is (m1 position, m2 position)\n",
    "    return (int(position_tuple[0]//5), int(position_tuple[1]//5))\n",
    "\n",
    "def convert_2d_index_to_motor_positions(index_tuple):\n",
    "    return (index_tuple[0]*5, index_tuple[1]*5)\n",
    "\n",
    "def convert_1d_index_to_2d_index(index, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return np.unravel_index(index, dimensions)\n",
    "\n",
    "def convert_2d_index_to_1d_index(index_tuple, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return np.ravel_multi_index(index_tuple, dimensions)\n",
    "\n",
    "def convert_motor_positions_to_1d_index(position_tuple, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return convert_2d_index_to_1d_index(convert_motor_positions_to_2d_index(position_tuple), dimensions)\n",
    "\n",
    "def convert_1d_index_to_motor_positions(index, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return convert_2d_index_to_motor_positions(convert_1d_index_to_2d_index(index, dimensions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd95a6-c7cd-4289-a462-10056a20e5bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Policy helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbef16a-77bb-47d7-99fe-44e36c1b2cd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1839ace-528a-400e-a5c7-cbc2cc4d16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_prob(actor_av_array, temperature=1):\n",
    "    # Divide all values by temperature\n",
    "    temperature_array = actor_av_array/temperature\n",
    "    \n",
    "    # Find max state value\n",
    "    max_value = np.max(actor_av_array)\n",
    "    \n",
    "    # Generate the numerator for each element by subtracting max value and exponentiating\n",
    "    numerator_array = actor_av_array - max_value\n",
    "    numerator_array = np.exp(numerator_array)\n",
    "\n",
    "    # Get the denominator by summing all values in the numerator\n",
    "    denominator_array = np.sum(numerator_array)\n",
    "    \n",
    "    \n",
    "    # Calculate the softmax value array and return to agent\n",
    "    softmax_array = numerator_array / denominator_array\n",
    "    \n",
    "    return softmax_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4710ccbe-0f6a-4d6b-9497-5455942a158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_avg_calc(value, last_value, window):\n",
    "    return (1/window)*value + (1-(1/window))*last_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748f703-5499-4dcb-8843-f213e728d516",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d769522c-bc81-4f81-9618-d8337978124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarEnv:\n",
    "    def __init__(self, reward_data_path, shape=(37,37), motor_power_draw_per_degree=MOTOR_ANGLE_POWER_DRAW, control=None):\n",
    "        self.shape = shape\n",
    "        self.reward_array = np.zeros(shape)\n",
    "        self.last_motor_position = (90,90)\n",
    "        self.motor_power_draw_per_degree = motor_power_draw_per_degree\n",
    "        # load in reward data\n",
    "        rewards = load_and_prep_data(reward_data_path)\n",
    "        for index, row in rewards.iterrows():\n",
    "            motor_1_index = int(row['motor_1_position'].item()//5)\n",
    "            motor_2_index = int(row['motor_2_position'].item()//5)\n",
    "            position_reward = row['power'].item()\n",
    "            self.reward_array[motor_1_index][motor_2_index] = position_reward\n",
    "        if control is not None:\n",
    "            # Control sets the entire env value array to a constant for debugging\n",
    "            self.reward_array = np.full(shape, control)\n",
    "    \n",
    "    # For operation\n",
    "    def calculate_motor_power_draw(self, new_position_tuple):\n",
    "        # new_position_tuple is the new motor position action requested by the agent\n",
    "        return self.motor_power_draw_per_degree * (abs(self.last_motor_position[0] - new_position_tuple[0]) + \\\n",
    "                                              abs(self.last_motor_position[1] - new_position_tuple[1]))\n",
    "    \n",
    "    def env_step(self, action):\n",
    "        # Take a motor position and return the reward, next motor positions\n",
    "        # action is motor positions\n",
    "        index_tuples = convert_motor_positions_to_2d_index(action)\n",
    "        power_gained = self.reward_array[index_tuples[0]][index_tuples[1]]\n",
    "        print(action)\n",
    "        power_used = self.calculate_motor_power_draw(new_position_tuple=action)\n",
    "        next_state = index_tuples\n",
    "        self.last_motor_position = action\n",
    "        return power_gained - power_used, next_state\n",
    "    \n",
    "    # For debugging\n",
    "    def get_reward_array(self):\n",
    "        return self.reward_array\n",
    "    \n",
    "    def get_env_shape(self):\n",
    "        return self.reward_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec059f9e-08a3-4211-8617-0a2285fc9384",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680ef40-22fa-445b-ad71-31b7c2a07567",
   "metadata": {},
   "source": [
    "The Agent is now a Softmax Actor Critic\n",
    "\n",
    "**Error**\n",
    "\n",
    "$$\\delta _{t} = R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}) - \\hat{v}(S_{t})$$\n",
    "\n",
    "**Avg Reward Update**\n",
    "\n",
    "$$\\bar{R} \\leftarrow \\bar{R} + \\alpha _{\\bar{R}} \\delta _{t}$$\n",
    "\n",
    "**Critic Update**\n",
    "\n",
    "$$\\hat{v}(S_{t},A_{t}) \\leftarrow \\hat{v}(S_{t},A_{t}) + \\alpha _{critic} \\delta _{t}$$\n",
    "\n",
    "**Actor Update**\n",
    "\n",
    "$$\\vec{\\theta}_{S_{t}} \\leftarrow \\vec{\\theta}_{S_{t}} + \\alpha_{\\theta} \\delta_{t}  [\\vec{x}_h(S_t,A_t) - \\vec{x}_{softmax_{S_t}}]$$\n",
    "\n",
    "**Reward Function**\n",
    "\n",
    "$$R = P_{solar} \\, - \\, P_{motor\\,draw}$$\n",
    "\n",
    "**Agent Parameters**\n",
    "\n",
    "* average reward step size: 0.001\n",
    "* actor step size: 0.1\n",
    "* critic step size: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469e872-b860-43ca-858f-6c185cb07c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarAgent:\n",
    "    def __init__(self, actor_step_size, critic_step_size, avg_reward_step_size, temperature_value, solar_env, random_seed=RANDOM_SEED):\n",
    "        # Set the env\n",
    "        self.env = solar_env\n",
    "        \n",
    "        # Set step sizes\n",
    "        self.actor_step_size = actor_step_size\n",
    "        self.critic_step_size = critic_step_size\n",
    "        self.avg_reward_step_size = avg_reward_step_size\n",
    "        self.temperature = temperature_value\n",
    "        \n",
    "        # Set up memory for the actor and critic\n",
    "        self.env_shape = self.env.get_env_shape()\n",
    "        self.critic_array = np.zeros(self.env_shape)\n",
    "        self.actor_array = np.zeros(self.env_shape)\n",
    "        self.actions_vector = np.array(range(0, convert_2d_index_to_1d_index(self.env_shape) + 1))\n",
    "        \n",
    "        # Set up fields for agent steps\n",
    "        self.random_generator = np.random.RandomState(random_seed) \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.state = None\n",
    "        self.last_reward = None\n",
    "        self.avg_reward = 0\n",
    "        self.step_softmax_prob = None\n",
    "    \n",
    "        # Set up tracking metric items\n",
    "        self.state_visits = np.zeros(self.env_shape)\n",
    "        self.total_energy = 0\n",
    "        self.rolling_power = 0\n",
    "        self.rolling_window = rolling_window\n",
    "        self.transition_dict = None\n",
    "    \n",
    "    # Agent Operation\n",
    "    # =============================================\n",
    "    \n",
    "    def agent_start(self):\n",
    "        # Initialize the agent\n",
    "        self.avg_reward = 0\n",
    "        self.last_state = convert_motor_positions_to_1d_index([90,90])\n",
    "        self.last_action = self.last_state\n",
    "        self.last_reward = 0\n",
    "        self.state, self.reward = self.env.env_step(self.last_state)\n",
    "    \n",
    "    def get_state_values(self, state):\n",
    "        converted_index = convert_motor_positions_to_index(state)\n",
    "        return self.state_values[converted_index[0]][converted_index[1]]\n",
    "    \n",
    "    def agent_policy(self):\n",
    "        softmax_prob_array = softmax_prob(self.actor_array, self.temperature)\n",
    "        \n",
    "        ### Resume here\n",
    "        # Trying to resolve ranom choice needing 1D vs 2D\n",
    "        chosen_action_str = self.random_generator.choice(self.actions_array.flatten(), p=softmax_prob_array.flatten())\n",
    "        # Format chosen action\n",
    "        chosen_action = convert_string_index(chosen_action_str)\n",
    "        \n",
    "        # save softmax_prob as it will be useful later when updating the Actor\n",
    "        self.step_softmax_prob = softmax_prob.copy()\n",
    "        \n",
    "        # On return, convert to motor positions\n",
    "        return convert_index_to_motor_positions(chosen_action)\n",
    "    \n",
    "    def agent_step(self):\n",
    "        # Make a policy decision\n",
    "        action = self.agent_policy()\n",
    "        \n",
    "        # Interact with the environment\n",
    "        reward, next_state = self.env.env_step(action)\n",
    "        \n",
    "        # Compute delta first\n",
    "        delta = reward - self.avg_reward + np.sum(self.critic_w[active_tiles]) - np.sum(self.critic_w[self.prev_tiles]\n",
    "        \n",
    "        \n",
    "        # TD Update\n",
    "        last_state_value = self.get_state_values(self.last_state)\n",
    "        new_state_value = self.get_state_values(next_state)\n",
    "        error_term = reward - last_state_value\n",
    "        scaling_term = (1 + np.tanh(np.abs(error_term/last_state_value)))**self.chi\n",
    "        last_state_index = convert_motor_positions_to_index(self.last_state)\n",
    "        self.state_values[last_state_index[0]][last_state_index[1]] = last_state_value + self.step_size * error_term * scaling_term\n",
    "        \n",
    "        # For debugging\n",
    "        self.transition_dict = {\n",
    "            'reward': reward,\n",
    "            'new_state_value': new_state_value,\n",
    "            'last_state_value': last_state_value,\n",
    "            'error_term': error_term,\n",
    "            'updated_value_estimate': last_state_value + self.step_size * error_term\n",
    "        }\n",
    "        \n",
    "        # Update internal variables\n",
    "        self.last_state = next_state\n",
    "        \n",
    "        # For tracking\n",
    "        self.total_energy += reward\n",
    "        self.rolling_power = rolling_avg_calc(reward, self.rolling_power, self.rolling_window)\n",
    "        self.state_visits[last_state_index[0]][last_state_index[1]] += 1\n",
    "        \n",
    "        # =================\n",
    "        angle, ang_vel = state\n",
    "\n",
    "        ### Use self.tc to get active_tiles using angle and ang_vel (1 line)\n",
    "        # active_tiles = ?    \n",
    "        # ----------------\n",
    "        # your code here\n",
    "        active_tiles = self.tc.get_tiles(angle, ang_vel)\n",
    "        \n",
    "        # ----------------\n",
    "\n",
    "        ### Compute delta using Equation (1) (1 line)\n",
    "        # delta = ?\n",
    "        # ----------------\n",
    "        # your code here\n",
    "        delta = reward - self.avg_reward + np.sum(self.critic_w[active_tiles]) - np.sum(self.critic_w[self.prev_tiles])\n",
    "        \n",
    "        # ----------------\n",
    "\n",
    "        ### update average reward using Equation (2) (1 line)\n",
    "        # self.avg_reward += ?\n",
    "        # ----------------\n",
    "        # your code here\n",
    "        self.avg_reward += self.avg_reward_step_size * delta\n",
    "        \n",
    "        # ----------------\n",
    "\n",
    "        # update critic weights using Equation (3) and (5) (1 line)\n",
    "        # self.critic_w[self.prev_tiles] += ?\n",
    "        # ----------------\n",
    "        # your code here\n",
    "        self.critic_w[self.prev_tiles] += self.critic_step_size * delta\n",
    "        \n",
    "        # ----------------\n",
    "\n",
    "        # update actor weights using Equation (4) and (6)\n",
    "        # We use self.softmax_prob saved from the previous timestep\n",
    "        # We leave it as an exercise to verify that the code below corresponds to the equation.\n",
    "        for a in self.actions:\n",
    "            if a == self.last_action:\n",
    "                self.actor_w[a][self.prev_tiles] += self.actor_step_size * delta * (1 - self.softmax_prob[a])\n",
    "            else:\n",
    "                self.actor_w[a][self.prev_tiles] += self.actor_step_size * delta * (0 - self.softmax_prob[a])\n",
    "\n",
    "        ### set current_action by calling self.agent_policy with active_tiles (1 line)\n",
    "        # current_action = ? \n",
    "        # ----------------\n",
    "        # your code here\n",
    "        current_action = self.agent_policy(active_tiles)\n",
    "        \n",
    "        # ----------------\n",
    "\n",
    "        self.prev_tiles = active_tiles\n",
    "        self.last_action = current_action\n",
    "\n",
    "        return self.last_action\n",
    "    \n",
    "    # Tracking\n",
    "    # =============================================\n",
    "    \n",
    "    def get_critic_array(self):\n",
    "        return self.critic_array.copy()\n",
    "    \n",
    "    def get_actor_array(self):\n",
    "        return self.actor_array.copy()\n",
    "    \n",
    "    def get_agent_energy(self):\n",
    "        return self.total_energy\n",
    "    \n",
    "    def get_agent_rolling_power(self):\n",
    "        return self.rolling_power\n",
    "    \n",
    "    def get_transition_dict(self):\n",
    "        return self.transition_dict\n",
    "    \n",
    "    def get_state_visits(self):\n",
    "        return self.state_visits.copy()\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d30d9-816c-4b97-a8f7-6e1817d70857",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c9354-8c79-49f3-8b0b-e0ac64cdc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value_error(agent_array, env_array):\n",
    "    return np.sqrt((agent_array - env_array)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b185c2-663c-4d9a-bc98-6d85460ffbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_dict_to_df(progress_dict):\n",
    "    dict_list = []\n",
    "    for x in progress_dict.keys():\n",
    "        temp_dict = progress_dict[x]\n",
    "        temp_dict['step'] = x\n",
    "        dict_list.append(temp_dict)\n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63427c-3527-4cd8-9051-195a85ebdc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_experiment(exp_agent, exp_env, steps, interval):\n",
    "    progress_dict = {}\n",
    "    exp_agent.agent_start()\n",
    "    progress_dict['0'] = {\n",
    "                'state_value': exp_agent.get_state_value_array(),\n",
    "                'rolling_power': exp_agent.get_agent_rolling_power(),\n",
    "                'mse': calculate_value_error(exp_agent.get_state_value_array(), exp_env.get_reward_array()),\n",
    "                'state_visits': exp_agent.get_state_visits(),\n",
    "                'total_energy': exp_agent.get_agent_energy()\n",
    "            }\n",
    "    for i in tqdm(range(1, steps + 1)):\n",
    "        exp_agent.agent_step()\n",
    "        if i%interval == 0:\n",
    "            progress_dict[str(i)] = {\n",
    "                'state_value': exp_agent.get_state_value_array(),\n",
    "                'rolling_power': exp_agent.get_agent_rolling_power(),\n",
    "                'mse': calculate_value_error(exp_agent.get_state_value_array(), exp_env.get_reward_array()),\n",
    "                'state_visits': exp_agent.get_state_visits(),\n",
    "                'total_energy': exp_agent.get_agent_energy()\n",
    "            }\n",
    "    progress_df = progress_dict_to_df(progress_dict)\n",
    "    return exp_agent, progress_dict, progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81dff5d-d828-495d-b50d-a9a3ae1a03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_power(progress_df, exp_env, height, width):\n",
    "    max_output = exp_env.get_reward_array().max()\n",
    "    progress_df['env_max'] = max_output\n",
    "    progress_df['optimal_energy'] = progress_df['step'].astype(float) * max_output\n",
    "    progress_df['difference'] = (progress_df['rolling_power'] - progress_df['env_max']) / progress_df['env_max']\n",
    "    make_subplots_plot(df=progress_df, x='step', subplot_group_list=[\n",
    "    {\n",
    "        'title': 'Reward Comparison (Agent vs Max)',\n",
    "         'columns': ['env_max','rolling_power']\n",
    "    },\n",
    "        {\n",
    "        'title': 'Energy Comparison (Agent vs Max)',\n",
    "         'columns': ['total_energy', 'optimal_energy']\n",
    "        }\n",
    "    ], height=height, width=width\n",
    "                 )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8327a61-ebab-4215-b354-77151b048ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subplots_plot(df, x, subplot_group_list, height=400, width=400, plot_title=''):\n",
    "    fig = make_subplots(rows=len(subplot_group_list), cols=1,\n",
    "                       subplot_titles=[x['title'] for x in subplot_group_list])\n",
    "    \n",
    "    for i in range(len(subplot_group_list)):\n",
    "        row = i+1\n",
    "        title = subplot_group_list[i]['title']\n",
    "        for column_name in subplot_group_list[i]['columns']:\n",
    "            fig.append_trace(go.Scatter(\n",
    "                x=df[x],\n",
    "                y=df[column_name], name=column_name\n",
    "            ), row=row, col=1)\n",
    "\n",
    "    fig.update_layout(height=height, width=width, title_text=plot_title)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0ed7f-fca1-4a86-90da-c2d4a8b76fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_array_evolution(exp_progress_dict, exp_interval, field, width_plot, height_plot, zmax=None, zmin=None):\n",
    "    matrix_list = [exp_progress_dict[x][field] for x in exp_progress_dict.keys()]\n",
    "    fig = go.Figure(\n",
    "        data=[go.Heatmap(z=matrix_list[0], zmax=zmax, zmin=zmin)],layout=go.Layout(\n",
    "                title=\"Step 0\",\n",
    "                updatemenus=[dict(\n",
    "                    type=\"buttons\",\n",
    "                    buttons=[dict(label=\"Play\",\n",
    "                                  method=\"animate\",\n",
    "                                  args=[None]),\n",
    "                            dict(label=\"Pause\",\n",
    "                                 method=\"animate\",\n",
    "                                 args=[None,\n",
    "                                       {\"frame\": {\"duration\": 0, \"redraw\": False},\n",
    "                                        \"mode\": \"immediate\",\n",
    "                                        \"transition\": {\"duration\": 0}}],\n",
    "                                 )])]\n",
    "            ),\n",
    "            frames=[go.Frame(data=[go.Heatmap(z=matrix_list[i])], \n",
    "                             layout=go.Layout(title_text=f\"Step {i * exp_interval}\")) for i in range(1, len(matrix_list))]\n",
    "    )\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    fig.update_layout({\n",
    "        'height': height_plot,\n",
    "        'width': width_plot}\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46258b2-b13c-41dd-9e5a-5ca28d90076c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Resume here with experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357ddf9-2eb9-4c27-aa7e-856c90a0544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting max/min\n",
    "zmax_plot = 0.015\n",
    "zmin_plot = 0\n",
    "width_plot = 400\n",
    "height_plot = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659ce2e-da1b-42a0-84d0-1ae7e6edf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment factors\n",
    "exp_env_control = None\n",
    "exp_data_path = '../../../rl_agent/simulation_data/data/corrected_motors/run_5_kitchen_no_lights.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59270191-0081-447e-b68c-80c6e3d5f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set runtime values\n",
    "steps = 100000\n",
    "interval = 1000\n",
    "\n",
    "# Agent factors\n",
    "exp_step_size = 0.1\n",
    "exp_epsilon = 0.05\n",
    "exp_chi = 1.1\n",
    "exp_initialization_value = 0.02\n",
    "rolling_avg_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d6043-850d-492b-9cf1-3724120b673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_env = SolarEnv(reward_data_path=exp_data_path, shape=(37,37), control=exp_env_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ecf1e-ec97-4b95-8392-f6c96078e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_env.env_step((90,90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a67299-2264-4a6e-bab9-1fc64e0da7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_env = SolarEnv(reward_data_path=exp_data_path, shape=(37,37), control=exp_env_control)\n",
    "experiment_agent = SolarAgent(step_size=exp_step_size, epsilon=exp_epsilon, \n",
    "                              chi=exp_chi, initialization_value=exp_initialization_value, \n",
    "                              env=experiment_env, rolling_window=rolling_avg_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf2922-97d1-475b-8339-96c152178b4c",
   "metadata": {},
   "source": [
    "#### Initiate an experiment\n",
    "Remember to re-initialize the agent above or else it will resume learning on the exisitng agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56169d-6131-46c9-b9b3-a790b79bd1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "experiment_agent, progress_dict, progress_df = run_agent_experiment(exp_agent=experiment_agent, \n",
    "                                                                   exp_env=experiment_env, steps=steps, interval=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7d3ee-fae0-46bb-9cae-858cc6f914fb",
   "metadata": {},
   "source": [
    "#### Show the agent difference from known optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e089d-ba49-432f-a365-8032603f1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_power(progress_df, experiment_env, height=600, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0d1857-2d03-4bb9-adf8-6e9b43e14862",
   "metadata": {},
   "source": [
    "#### Visualize state visits and learned values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ea419-aa62-4162-9975-ac1f318c2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_max_step = True\n",
    "specific_step = '100000'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8150cc-903e-4eba-bd7e-6d86af1c8312",
   "metadata": {},
   "source": [
    "State visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca816f-012b-4e4c-8c7f-4f0b0417b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(progress_dict[(specific_step if not do_max_step else str(steps))]['state_visits'], width=width_plot, height=height_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ff4a1-fa21-475c-86c1-0ca6c6b971e9",
   "metadata": {},
   "source": [
    "State values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0630444-7a92-405c-9acc-6d8dab7e2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(progress_dict[(specific_step if not do_max_step else str(steps))]['state_value'], width=width_plot, \n",
    "          height=height_plot, zmin=zmin_plot, zmax=zmax_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0945d39-f673-4897-97c2-aa2fdb125d2a",
   "metadata": {},
   "source": [
    "#### Environment True Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c2d0f-778b-4b03-9983-0d22fadb977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(experiment_env.get_reward_array(), width=width_plot, height=height_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4eaa99-d91e-4ca2-a5cc-1c0e5470661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array_evolution(progress_dict, interval, 'state_visits', width_plot, height_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ed37e-28a0-472b-a036-714c7dc1a198",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visulaize evolution of learned state value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6398b-6be2-4f06-839c-d426b342c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array_evolution(progress_dict, interval, 'state_value', width_plot, height_plot, zmax_plot, zmin_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3221bf-843c-4766-b827-1682a3bf7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE vs steps\n",
    "px.scatter(progress_df, x='step', y='rolling_power', width=width_plot, height=height_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72827c36-139e-4032-8eab-65d772cb6d04",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff4000-d540-4893-a785-11236d03159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request codes\n",
    "MOTOR_CONTROL = 1000\n",
    "STATE_REQUEST = 2000\n",
    "RESET_CODE = 6666\n",
    "\n",
    "def scan_space(arduino):\n",
    "    # Run start\n",
    "    run_start = time.time()\n",
    "    data_dict_list = []\n",
    "    last_motor_interval = 0\n",
    "    last_measure_interval = -1\n",
    "    motor_frequency = 2\n",
    "    measure_frequency = 1\n",
    "    # Set timeouts\n",
    "    abort = False\n",
    "    \n",
    "    for xy_degree in range(0, 181, 5):\n",
    "        for yz_degree in range(0, 181, 5):\n",
    "            si.write_serial_line(arduino, [MOTOR_CONTROL, xy_degree, yz_degree], print_message=False)\n",
    "            new_message, abort = si.listen_for_serial(arduino)\n",
    "            if new_message is not None and not abort:\n",
    "                data_dict_list.append(new_message)\n",
    "            elif abort:\n",
    "                break\n",
    "            else:\n",
    "                print('Empty message received without abort issue')\n",
    "            time.sleep(0.1) # Wait for steady state\n",
    "        if abort:\n",
    "            break\n",
    "        print('xy:',xy_degree,'yz:',yz_degree)\n",
    "    # Write back to start state\n",
    "    write_serial_line(arduino, [si.MOTOR_CONTROL, 90, 90])\n",
    "\n",
    "    return pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07994661-9e43-4372-b543-b2641c5cad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('\\nARDUINO CONTROL TESTING')\n",
    "    print('-------------------------')\n",
    "    # Initialize serial port\n",
    "    print('\\nIniitalizing device...')\n",
    "    serial_port = '/dev/cu.usbmodem14101'\n",
    "    baud_rate = 9600\n",
    "    timeout = 5\n",
    "    arduino = si.initialize_serial(serial_port=serial_port, baud_rate=baud_rate, timeout=timeout)\n",
    "    print('\\t - SUCCESS: Device initialized.')\n",
    "    \n",
    "    si.write_serial_line(arduino, [MOTOR_CONTROL, 90, 90])\n",
    "\n",
    "    # Run a loop where motor position incremented every 5 seconds, print out message\n",
    "    print('\\nBeginning loop sequence...')\n",
    "#     data = scan_space(arduino)\n",
    "    print('\\t - Loop complete.')\n",
    "\n",
    "    # Add relative time to returned data and print out\n",
    "#     data['t_relative'] = data['timestamp'] - data['timestamp'].iloc[0]\n",
    "    print('\\nData broadcasted by Arduino:\\n')\n",
    "    \n",
    "#     data.to_csv('/Users/jackogrady/Git/rl-solar/rl_agent/simulation_data/data/run_6_kitchen_no_lights_swapped_motors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55694cd-2430-4233-943d-51758fc16cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1b648-e1a8-4d7d-84ca-56d3c3b878eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_serial_line(arduino, [1000, 180, 90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e08c6-02c9-4216-80d9-4968f08b292b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
