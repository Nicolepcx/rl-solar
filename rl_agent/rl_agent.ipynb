{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8652521c-aa5e-4859-aa7f-c77f6bf07448",
   "metadata": {},
   "source": [
    "# Solar RL Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22e5db-7581-44e1-8a32-5eac938bf1d7",
   "metadata": {},
   "source": [
    "Softmax Actor-Critic for continuous task of positioning solar array to maximize energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624225a-6ec1-4253-9310-fa08c4252c2a",
   "metadata": {},
   "source": [
    "### Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd871a59-1985-4db5-a7fe-7dc62869533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a165287-0f30-409d-bc0a-53caea404fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare random seed\n",
    "RANDOM_SEED = 1\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set default constants for problem\n",
    "ARRAY_DIMENSION_TUPLE = (37,37)\n",
    "MOTOR_ANGLE_POWER_DRAW = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2034f5c1-bc8c-43a4-ad09-2a0b2bf8456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder environment, hyper-parameter study imports\n",
    "from solar_env import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ddd73-04c6-431b-86f3-e6405e2cf82b",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b21653-0b29-444c-be92-b13592e183fd",
   "metadata": {},
   "source": [
    "#### Non-Agent Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de91c5da-6b55-4f98-b581-8e58fc408c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_avg_calc(value, last_value, window):\n",
    "    return (1/window)*value + (1-(1/window))*last_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906d26d-0fe3-45d0-9b8c-288e48429475",
   "metadata": {},
   "source": [
    "#### Agent Related"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55722c9-3c73-4689-a92e-e24c43df0d57",
   "metadata": {},
   "source": [
    "Converting index, motor positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d11c553-cae0-43af-b8fb-d6ee79cf1c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_motor_positions_to_2d_index(position_tuple):\n",
    "    # position tuple is (m1 position, m2 position)\n",
    "    return (int(position_tuple[0]//5), int(position_tuple[1]//5))\n",
    "\n",
    "def convert_2d_index_to_motor_positions(index_tuple):\n",
    "    return (index_tuple[0]*5, index_tuple[1]*5)\n",
    "\n",
    "def convert_1d_index_to_2d_index(index, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return np.unravel_index(index, dimensions)\n",
    "\n",
    "def convert_2d_index_to_1d_index(index_tuple, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return np.ravel_multi_index(index_tuple, dimensions)\n",
    "\n",
    "def convert_motor_positions_to_1d_index(position_tuple, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return convert_2d_index_to_1d_index(convert_motor_positions_to_2d_index(position_tuple), dimensions)\n",
    "\n",
    "def convert_1d_index_to_motor_positions(index, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return convert_2d_index_to_motor_positions(convert_1d_index_to_2d_index(index, dimensions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cced13b-5793-4f3f-8ebf-a04b39539385",
   "metadata": {},
   "source": [
    "Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "299941bf-4318-465e-9f19-42a0efa42229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_prob(actor_av_array, temperature=1):\n",
    "    # Divide all values by temperature\n",
    "    temperature_array = actor_av_array/temperature\n",
    "    \n",
    "    # Find max state value\n",
    "    max_value = np.max(temperature_array)\n",
    "    \n",
    "    # Generate the numerator for each element by subtracting max value and exponentiating\n",
    "    numerator_array = temperature_array - max_value\n",
    "    numerator_array = np.exp(numerator_array)\n",
    "\n",
    "    # Get the denominator by summing all values in the numerator\n",
    "    denominator_array = np.sum(numerator_array)\n",
    "    \n",
    "    \n",
    "    # Calculate the softmax value array and return to agent\n",
    "    softmax_array = numerator_array / denominator_array\n",
    "    \n",
    "    return softmax_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98d42d-cffc-4990-a107-ccf116e8a80f",
   "metadata": {},
   "source": [
    "## Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5ad83b9-6d9b-4c16-b181-7ae32c268a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxAgent:\n",
    "    def __init__(self, actor_step_size, critic_step_size, avg_reward_step_size, temperature_value, env_shape, reward_rolling_avg_window, \n",
    "                 random_seed=RANDOM_SEED, actor_init_value=None, critic_init_value=None):\n",
    "        # Set step sizes\n",
    "        self.actor_step_size = actor_step_size\n",
    "        self.critic_step_size = critic_step_size\n",
    "        self.avg_reward_step_size = avg_reward_step_size\n",
    "        self.temperature = temperature_value\n",
    "        \n",
    "        # Set up memory for the actor and critic\n",
    "        self.env_shape = env_shape\n",
    "        max_index_2d = self.env_shape[0] - 1\n",
    "        max_index_1d = convert_2d_index_to_1d_index((max_index_2d, max_index_2d), dimensions=self.env_shape)\n",
    "        self.agent_shape = (max_index_1d + 1, max_index_1d + 1)\n",
    "        \n",
    "        # Set init values of actor and critic\n",
    "        # Actor\n",
    "        if actor_init_value:\n",
    "            self.actor_array = np.full(self.agent_shape, actor_init_value)\n",
    "        else:\n",
    "            self.actor_array = np.zeros(self.agent_shape)\n",
    "        # Critic\n",
    "        if critic_init_value:\n",
    "            self.critic_array = np.full(self.agent_shape, critic_init_value)\n",
    "        else:\n",
    "            self.critic_array = np.zeros(self.agent_shape)\n",
    "        \n",
    "        # Create the actions and feature vectors\n",
    "        self.actions_vector = np.array(range(0, max_index_1d + 1))\n",
    "        self.base_feature_vector = np.zeros(max_index_1d + 1)\n",
    "        \n",
    "        # Set up fields for agent steps -- all agent internal functions are 1d index\n",
    "        self.random_generator = np.random.RandomState(random_seed) \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.state = None\n",
    "        self.last_reward = None\n",
    "        self.avg_reward = 0\n",
    "        self.step_softmax_prob = None\n",
    "    \n",
    "        # Set up tracking metric items\n",
    "        self.state_visits = np.zeros(self.env_shape) # track in 2d to better map to env map\n",
    "        self.total_reward = 0\n",
    "        self.rolling_reward = 0\n",
    "        self.rolling_window = reward_rolling_avg_window\n",
    "        self.transition_dict = None\n",
    "    \n",
    "    # Agent Operation\n",
    "    # =============================================\n",
    "    def agent_start(self):\n",
    "        # Initialize the agent\n",
    "        self.avg_reward = 0\n",
    "        self.last_state = self.agent_shape[0]//2  # Set to the middle state for init\n",
    "        self.last_action = self.last_state\n",
    "        self.last_reward = 0\n",
    "        \n",
    "        # For tracking\n",
    "        self.state_visits[convert_1d_index_to_2d_index(self.last_state, dimensions=self.env_shape)] += 1\n",
    "    \n",
    "    \n",
    "    def agent_policy(self):\n",
    "        # Compute the softmax prob for actions in given state\n",
    "        softmax_prob_array = softmax_prob(self.actor_array[self.last_state], self.temperature)\n",
    "        \n",
    "        # Overlay the softmax probs onto actions vector\n",
    "        chosen_action = self.random_generator.choice(self.actions_vector, p=softmax_prob_array)\n",
    "        \n",
    "        # save softmax_prob as it will be useful later when updating the Actor\n",
    "        self.step_softmax_prob = softmax_prob_array.copy()\n",
    "        self.last_action = chosen_action\n",
    "        \n",
    "        # Return the 1d index of action\n",
    "        return chosen_action\n",
    "        \n",
    "    \n",
    "    def agent_step(self, reward, next_state):\n",
    "        # Compute delta\n",
    "        delta = reward - self.avg_reward + np.mean(self.critic_array[next_state]) - np.mean(self.critic_array[self.last_state])\n",
    "        \n",
    "        # Update avg reward\n",
    "        self.avg_reward += self.avg_reward_step_size * delta\n",
    "        \n",
    "        # Update critic weights\n",
    "        self.critic_array[self.last_state][self.last_action] += self.critic_step_size * delta\n",
    "        \n",
    "        # Update actor weights\n",
    "        feature_vector = self.base_feature_vector.copy() # copy the zeros vector\n",
    "        feature_vector[self.last_action] = 1 # set last action to one\n",
    "        self.actor_array[self.last_state] += self.actor_step_size * delta * (feature_vector - self.step_softmax_prob)\n",
    "        \n",
    "        # Update last state, etc\n",
    "        self.last_state = next_state\n",
    "        \n",
    "        # For tracking\n",
    "        self.total_reward += reward\n",
    "        self.rolling_reward = rolling_avg_calc(reward, self.rolling_reward, self.rolling_window)\n",
    "        self.state_visits[convert_1d_index_to_2d_index(self.last_state, self.env_shape)] += 1\n",
    "        self.last_delta = delta\n",
    "    \n",
    "    # Tracking\n",
    "    # =============================================\n",
    "    \n",
    "    def get_critic_array(self):\n",
    "        return self.critic_array.copy()\n",
    "    \n",
    "    def get_actor_array(self):\n",
    "        return self.actor_array.copy()\n",
    "    \n",
    "    def set_actor_array(self, array):\n",
    "        self.actor_array = array\n",
    "        \n",
    "    def get_actions_vector(self):\n",
    "        return self.actions_vector\n",
    "    \n",
    "    def get_agent_avg_reward(self):\n",
    "        return self.avg_reward\n",
    "    \n",
    "    def get_agent_last_delta(self):\n",
    "        return self.last_delta\n",
    "    \n",
    "    def get_agent_last_state(self):\n",
    "        return self.last_state\n",
    "    \n",
    "    def get_base_feature_vector(self):\n",
    "        return self.base_feature_vector\n",
    "    \n",
    "    def get_agent_total_reward(self):\n",
    "        return self.total_reward\n",
    "    \n",
    "    def get_agent_rolling_reward(self):\n",
    "        return self.rolling_reward\n",
    "    \n",
    "    def get_state_visits(self):\n",
    "        return self.state_visits.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2123d6c1-b1f9-4c13-97ba-db4799848f38",
   "metadata": {},
   "source": [
    "## Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "587d5281-ad3d-411d-8dc1-c67eb4d9af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data_path = 'simulation_data/data/corrected_motors/run_5_kitchen_no_lights.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1435e40d-49b2-4603-893e-f22b1f44f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = load_and_format_solar_df(experiment_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "711b5227-112b-4934-a4f5-a00d67ec3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = convert_solar_df_to_value_array(data_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2d40f-7c85-44b0-b186-446aa3259732",
   "metadata": {},
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c11084-e8bd-47bf-acc1-cc389334dc8f",
   "metadata": {},
   "source": [
    "### Experiment Runtime Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7622d117-2349-4a5d-ac20-d2c15569e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run individual step of experiment\n",
    "def run_experiment_step(env:SolarEnv, agent:SoftmaxAgent, step):\n",
    "    \"\"\"\n",
    "    Carry out one step of interaction between agent and environment\n",
    "    \n",
    "    Args:\n",
    "        env (SolarEnv): the environment being used in the experiment\n",
    "        agent (SoftmaxAgent): the agent being used in the experiment\n",
    "        step (int): the step number of the experiment\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    action = agent.agent_policy()\n",
    "    reward, next_state_tuple = env.env_step(convert_1d_index_to_2d_index(action, env.get_env_shape()), \n",
    "                                            convert_1d_index_to_2d_index(agent.get_agent_last_state(), env.get_env_shape()))\n",
    "    agent.agent_step(reward, convert_2d_index_to_1d_index(next_state_tuple, env.get_env_shape()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "869fd962-c1d5-4c7b-bb85-971f58d3e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tracking_dict(step, env:SolarEnv, agent:SoftmaxAgent):\n",
    "    \"\"\"\n",
    "    Generate a tracking dict for a given step of an experiment\n",
    "    \n",
    "    Args:\n",
    "        step (int): the step number of an experiment\n",
    "        env (SolarEnv): the environment of an experiment\n",
    "        agent (SoftmaxAgent): the agent of an experiment\n",
    "    Returns:\n",
    "        dict: a tracking dict with keys seen in function below\n",
    "    \"\"\"\n",
    "    \n",
    "    tracking_dict = {\n",
    "        'step': step,\n",
    "        'state_value': agent.get_critic_array(),\n",
    "        'action_prob': agent.get_actor_array(),\n",
    "        'rolling_power': agent.get_agent_rolling_power(),\n",
    "        'state_visits': agent.get_state_visits(),\n",
    "        'total_energy': agent.get_agent_energy(),\n",
    "        'env_rewards': env.get_reward_array()\n",
    "    }\n",
    "    return tracking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4623515d-602b-4fe5-83ff-167f09330121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_experiment(environment:SolarEnv, steps, seed, actor_step_size, critic_step_size, \n",
    "                         avg_reward_step_size, temperature, rolling_steps_measurement=10, \n",
    "                         logging_interval=1000):\n",
    "    \"\"\"\n",
    "    Run an end-to-end experiment with the agent and determine the total reward during the experiment\n",
    "    \n",
    "    Args:\n",
    "        environment (SolarEnv): The environment class for the agent to interact with\n",
    "        steps (int): The number of steps to run the experiment for\n",
    "        seed (int): The random seed number to use for the agent\n",
    "        actor_step_size (float): Step-size parameter for actor in agent\n",
    "        critic_step_size (float): Step-size parameter for critic in agent\n",
    "        avg_reward_step_size (float): Step-size parameter for avg reward in agent\n",
    "        temperature (float): Temperature parameter for actor policy\n",
    "    Kwargs:\n",
    "        rolling_steps_measurement (int): For tracking, the rolling avg steps for calculating running power from agent\n",
    "        logging_interval (int): Frequency of steps to log metrics from experiment, None to avoid logging\n",
    "    Returns:\n",
    "        float, DataFrame: the total reward the agent achieved in experiment, the tracking df of results in steps\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create agent with properties\n",
    "    experiment_agent = SoftmaxAgent(actor_step_size=actor_step_size, critic_step_size=critic_step_size,\n",
    "                                avg_reward_step_size=avg_reward_step_size,\n",
    "                                temperature_value=temperature, env_shape=environment.get_env_shape(), \n",
    "                                reward_rolling_avg_window=rolling_steps_measurement, random_seed=seed)\n",
    "    # Initialize Agent\n",
    "    experiment_agent.agent_start()\n",
    "    \n",
    "    # Initialize a tradcking dict\n",
    "    tracking_dict_list = {}\n",
    "    tracking_dict_list.append(create_tracking_dict(step=0))\n",
    "    \n",
    "    # Only do one conditional logging check to improve runtime\n",
    "    if logging_interval is not None:\n",
    "        # Run specified number of steps\n",
    "        for i in range(1, steps + 1):\n",
    "            run_experiment_step(environment, experiment_agent, step=i)\n",
    "            if i % logging_interval == 0:\n",
    "                tracking_dict_list.append(create_tracking_dict(step=i))\n",
    "        tracking_df = pd.DataFrame(tracking_dict_list)\n",
    "    \n",
    "    # If no logging, just run the experiment straight\n",
    "    else:\n",
    "        for i in range(1, steps + 1):\n",
    "            run_experiment_step(environment, experiment_agent, step=i)\n",
    "        tracking_df = pd.DataFrame() # empty df\n",
    "    \n",
    "    # Return total reward from experiment\n",
    "    return experiment_agent.get_agent_total_reward(), tracking_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12a019-327a-4839-a0e5-0fb57a3380fc",
   "metadata": {},
   "source": [
    "### Hyperparameter Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "204e9f9d-1564-49a4-9832-9e186a29788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparam_study(environment:SolarEnv, steps, seed, \n",
    "                         temperature_values=[1*10**-x for x in range(0,5)], \n",
    "                         actor_step_size_values=[1*10**-x for x in range(0,5)], \n",
    "                         critic_step_size_values=[1*10**-x for x in range(0,5)], \n",
    "                         avg_reward_step_size_values=[1*10**-x for x in range(0,5)]):\n",
    "    \"\"\"\n",
    "    Conduct a hyperparameter study\n",
    "    \n",
    "    Args:\n",
    "        environment (SolarEnv): the environment to use for the study\n",
    "        steps (int): the number of steps to run each set of hyperparameters for in an experiment\n",
    "        seed (int): the random seed number to use for agent policy\n",
    "    Kwargs:\n",
    "        temperature_values (list): A list of values to study for temperature\n",
    "        actor_step_size_values: A list of values to study for actor step size\n",
    "        critic_step_size_values: A list of values to study for critic step size\n",
    "        avg_reward_step_size_values: A list of values to study for avg reward step size\n",
    "    Returns:\n",
    "        DataFrame: A dataframe of hyperparameters and the reward they achieved in an experiment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dict list to hold all unique combinations of hyperparameters\n",
    "    combinations_dict_list = []\n",
    "    # Sweep temperature\n",
    "    for temperature in temperature_values:\n",
    "        # Sweep actor step size\n",
    "        for actor_step_size in actor_step_size_values:\n",
    "            # Sweep critic step size\n",
    "            for critic_step_size in critic_step_size_values:\n",
    "                # Sweep avg reward step size\n",
    "                for avg_reward_step_size in avg_reward_step_size_values:\n",
    "                    combinations_dict_list.append(\n",
    "                        {'temperature': temperature,\n",
    "                         'actor_step_size': actor_step_size,\n",
    "                         'critic_step_size': critic_step_size,\n",
    "                         'avg_reward_step_size': avg_reward_step_size\n",
    "                        }\n",
    "                    )\n",
    "    # Create another dict list to hold results\n",
    "    results_dict_list = []\n",
    "    for combination in tqdm(combinations_dict_list):\n",
    "        total_reward = run_agent_experiment(environment, steps, seed, flip_mode, **combination)\n",
    "        combination['total_reward'] = total_reward\n",
    "        results_dict_list.append(combination)\n",
    "    # Return a DataFrame of all the results\n",
    "    return pd.DataFrame(results_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856f0d3-f06c-4950-a3b4-a6340dd5e106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
