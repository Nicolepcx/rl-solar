{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8652521c-aa5e-4859-aa7f-c77f6bf07448",
   "metadata": {},
   "source": [
    "# Solar RL Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22e5db-7581-44e1-8a32-5eac938bf1d7",
   "metadata": {},
   "source": [
    "Softmax Actor-Critic for continuous task of positioning solar array to maximize energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624225a-6ec1-4253-9310-fa08c4252c2a",
   "metadata": {},
   "source": [
    "### Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd871a59-1985-4db5-a7fe-7dc62869533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a165287-0f30-409d-bc0a-53caea404fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare random seed\n",
    "RANDOM_SEED = 1\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set default constants for problem\n",
    "ARRAY_DIMENSION_TUPLE = (37,37)\n",
    "MOTOR_ANGLE_POWER_DRAW = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2034f5c1-bc8c-43a4-ad09-2a0b2bf8456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder environment, hyper-parameter study imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ddd73-04c6-431b-86f3-e6405e2cf82b",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b21653-0b29-444c-be92-b13592e183fd",
   "metadata": {},
   "source": [
    "#### Non-Agent Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de91c5da-6b55-4f98-b581-8e58fc408c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_avg_calc(value, last_value, window):\n",
    "    return (1/window)*value + (1-(1/window))*last_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906d26d-0fe3-45d0-9b8c-288e48429475",
   "metadata": {},
   "source": [
    "#### Agent Related"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55722c9-3c73-4689-a92e-e24c43df0d57",
   "metadata": {},
   "source": [
    "Converting index, motor positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d11c553-cae0-43af-b8fb-d6ee79cf1c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_motor_positions_to_2d_index(position_tuple):\n",
    "    # position tuple is (m1 position, m2 position)\n",
    "    return (int(position_tuple[0]//5), int(position_tuple[1]//5))\n",
    "\n",
    "def convert_2d_index_to_motor_positions(index_tuple):\n",
    "    return (index_tuple[0]*5, index_tuple[1]*5)\n",
    "\n",
    "def convert_1d_index_to_2d_index(index, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return np.unravel_index(index, dimensions)\n",
    "\n",
    "def convert_2d_index_to_1d_index(index_tuple, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return np.ravel_multi_index(index_tuple, dimensions)\n",
    "\n",
    "def convert_motor_positions_to_1d_index(position_tuple, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return convert_2d_index_to_1d_index(convert_motor_positions_to_2d_index(position_tuple), dimensions)\n",
    "\n",
    "def convert_1d_index_to_motor_positions(index, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return convert_2d_index_to_motor_positions(convert_1d_index_to_2d_index(index, dimensions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cced13b-5793-4f3f-8ebf-a04b39539385",
   "metadata": {},
   "source": [
    "Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "299941bf-4318-465e-9f19-42a0efa42229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_prob(actor_av_array, temperature=1):\n",
    "    # Divide all values by temperature\n",
    "    temperature_array = actor_av_array/temperature\n",
    "    \n",
    "    # Find max state value\n",
    "    max_value = np.max(temperature_array)\n",
    "    \n",
    "    # Generate the numerator for each element by subtracting max value and exponentiating\n",
    "    numerator_array = temperature_array - max_value\n",
    "    numerator_array = np.exp(numerator_array)\n",
    "\n",
    "    # Get the denominator by summing all values in the numerator\n",
    "    denominator_array = np.sum(numerator_array)\n",
    "    \n",
    "    \n",
    "    # Calculate the softmax value array and return to agent\n",
    "    softmax_array = numerator_array / denominator_array\n",
    "    \n",
    "    return softmax_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98d42d-cffc-4990-a107-ccf116e8a80f",
   "metadata": {},
   "source": [
    "## Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ad83b9-6d9b-4c16-b181-7ae32c268a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxAgent:\n",
    "    def __init__(self, actor_step_size, critic_step_size, avg_reward_step_size, temperature_value, env_shape, reward_rolling_avg_window, \n",
    "                 random_seed=RANDOM_SEED, actor_init_value=None, critic_init_value=None):\n",
    "        # Set step sizes\n",
    "        self.actor_step_size = actor_step_size\n",
    "        self.critic_step_size = critic_step_size\n",
    "        self.avg_reward_step_size = avg_reward_step_size\n",
    "        self.temperature = temperature_value\n",
    "        \n",
    "        # Set up memory for the actor and critic\n",
    "        self.env_shape = env_shape\n",
    "        max_index_2d = self.env_shape[0] - 1\n",
    "        max_index_1d = convert_2d_index_to_1d_index((max_index_2d, max_index_2d), dimensions=self.env_shape)\n",
    "        self.agent_shape = (max_index_1d + 1, max_index_1d + 1)\n",
    "        \n",
    "        # Set init values of actor and critic\n",
    "        # Actor\n",
    "        if actor_init_value:\n",
    "            self.actor_array = np.full(self.agent_shape, actor_init_value)\n",
    "        else:\n",
    "            self.actor_array = np.zeros(self.agent_shape)\n",
    "        # Critic\n",
    "        if critic_init_value:\n",
    "            self.critic_array = np.full(self.agent_shape, critic_init_value)\n",
    "        else:\n",
    "            self.critic_array = np.zeros(self.agent_shape)\n",
    "        \n",
    "        # Create the actions and feature vectors\n",
    "        self.actions_vector = np.array(range(0, max_index_1d + 1))\n",
    "        self.base_feature_vector = np.zeros(max_index_1d + 1)\n",
    "        \n",
    "        # Set up fields for agent steps -- all agent internal functions are 1d index\n",
    "        self.random_generator = np.random.RandomState(random_seed) \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.state = None\n",
    "        self.last_reward = None\n",
    "        self.avg_reward = 0\n",
    "        self.step_softmax_prob = None\n",
    "    \n",
    "        # Set up tracking metric items\n",
    "        self.state_visits = np.zeros(self.env_shape) # track in 2d to better map to env map\n",
    "        self.total_reward = 0\n",
    "        self.rolling_reward = 0\n",
    "        self.rolling_window = reward_rolling_avg_window\n",
    "        self.transition_dict = None\n",
    "    \n",
    "    # Agent Operation\n",
    "    # =============================================\n",
    "    def agent_start(self):\n",
    "        # Initialize the agent\n",
    "        self.avg_reward = 0\n",
    "        self.last_state = self.agent_shape[0]//2  # Set to the middle state for init\n",
    "        self.last_action = self.last_state\n",
    "        self.last_reward = 0\n",
    "        \n",
    "        # For tracking\n",
    "        self.state_visits[convert_1d_index_to_2d_index(self.last_state, dimensions=self.env_shape)] += 1\n",
    "    \n",
    "    \n",
    "    def agent_policy(self):\n",
    "        # Compute the softmax prob for actions in given state\n",
    "        softmax_prob_array = softmax_prob(self.actor_array[self.last_state], self.temperature)\n",
    "        \n",
    "        # Overlay the softmax probs onto actions vector\n",
    "        chosen_action = self.random_generator.choice(self.actions_vector, p=softmax_prob_array)\n",
    "        \n",
    "        # save softmax_prob as it will be useful later when updating the Actor\n",
    "        self.step_softmax_prob = softmax_prob_array.copy()\n",
    "        self.last_action = chosen_action\n",
    "        \n",
    "        # Return the 1d index of action\n",
    "        return chosen_action\n",
    "        \n",
    "    \n",
    "    def agent_step(self, reward, next_state):\n",
    "        # Compute delta\n",
    "        delta = reward - self.avg_reward + np.mean(self.critic_array[next_state]) - np.mean(self.critic_array[self.last_state])\n",
    "        \n",
    "        # Update avg reward\n",
    "        self.avg_reward += self.avg_reward_step_size * delta\n",
    "        \n",
    "        # Update critic weights\n",
    "        self.critic_array[self.last_state][self.last_action] += self.critic_step_size * delta\n",
    "        \n",
    "        # Update actor weights\n",
    "        feature_vector = self.base_feature_vector.copy() # copy the zeros vector\n",
    "        feature_vector[self.last_action] = 1 # set last action to one\n",
    "        self.actor_array[self.last_state] += self.actor_step_size * delta * (feature_vector - self.step_softmax_prob)\n",
    "        \n",
    "        # Update last state, etc\n",
    "        self.last_state = next_state\n",
    "        \n",
    "        # For tracking\n",
    "        self.total_reward += reward\n",
    "        self.rolling_reward = rolling_avg_calc(reward, self.rolling_reward, self.rolling_window)\n",
    "        self.state_visits[convert_1d_index_to_2d_index(self.last_state, self.env_shape)] += 1\n",
    "        self.last_delta = delta\n",
    "    \n",
    "    # Tracking\n",
    "    # =============================================\n",
    "    \n",
    "    def get_critic_array(self):\n",
    "        return self.critic_array.copy()\n",
    "    \n",
    "    def get_actor_array(self):\n",
    "        return self.actor_array.copy()\n",
    "    \n",
    "    def set_actor_array(self, array):\n",
    "        self.actor_array = array\n",
    "        \n",
    "    def get_actions_vector(self):\n",
    "        return self.actions_vector\n",
    "    \n",
    "    def get_agent_avg_reward(self):\n",
    "        return self.avg_reward\n",
    "    \n",
    "    def get_agent_last_delta(self):\n",
    "        return self.last_delta\n",
    "    \n",
    "    def get_agent_last_state(self):\n",
    "        return self.last_state\n",
    "    \n",
    "    def get_base_feature_vector(self):\n",
    "        return self.base_feature_vector\n",
    "    \n",
    "    def get_agent_total_reward(self):\n",
    "        return self.total_reward\n",
    "    \n",
    "    def get_agent_rolling_reward(self):\n",
    "        return self.rolling_reward\n",
    "    \n",
    "    def get_state_visits(self):\n",
    "        return self.state_visits.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2123d6c1-b1f9-4c13-97ba-db4799848f38",
   "metadata": {},
   "source": [
    "## Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "587d5281-ad3d-411d-8dc1-c67eb4d9af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2d40f-7c85-44b0-b186-446aa3259732",
   "metadata": {},
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c11084-e8bd-47bf-acc1-cc389334dc8f",
   "metadata": {},
   "source": [
    "### Experiment Runtime Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7622d117-2349-4a5d-ac20-d2c15569e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run individual step of experiment\n",
    "def run_experiment_step(env:TestEnv, agent:SoftmaxAgent, step):\n",
    "    action = agent.agent_policy()\n",
    "    reward, next_state_tuple = env.env_step(convert_1d_index_to_2d_index(action, env.get_env_shape()), \n",
    "                                            convert_1d_index_to_2d_index(agent.get_agent_last_state(), env.get_env_shape()))\n",
    "    agent.agent_step(reward, convert_2d_index_to_1d_index(next_state_tuple, env.get_env_shape()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4623515d-602b-4fe5-83ff-167f09330121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_experiment(environment, steps, seed, flip_mode,\n",
    "                         actor_step_size, critic_step_size, avg_reward_step_size, temperature,\n",
    "                         rolling_steps_measurement=10):\n",
    "    # Create agent with properties\n",
    "    test_agent = SoftmaxAgent(actor_step_size=actor_step_size, critic_step_size=critic_step_size,\n",
    "                                avg_reward_step_size=avg_reward_step_size,\n",
    "                                temperature_value=temperature, env_shape=environment.get_env_shape(), \n",
    "                                reward_rolling_avg_window=rolling_steps_measurement, random_seed=seed)\n",
    "    test_agent.agent_start()\n",
    "    \n",
    "    # Static environment\n",
    "    if flip_mode == None:\n",
    "        for i in range(steps):\n",
    "            run_experiment_step(environment, test_agent, step=i)\n",
    "    \n",
    "    # Changing environment\n",
    "    elif flip_mode == 'roll':\n",
    "        study_reward_array = environment.get_reward_array()\n",
    "        interval = 500\n",
    "        current_env = TestEnv(study_reward_array, movement_penalty=0.1)\n",
    "        for i in range(steps):\n",
    "            if i % interval == 0 and i != 0:\n",
    "                current_env = TestEnv(np.roll(study_reward_array, i//interval), movement_penalty=0.1)\n",
    "            run_experiment_step(current_env, test_agent, step=i)\n",
    "    return test_agent.get_agent_total_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12a019-327a-4839-a0e5-0fb57a3380fc",
   "metadata": {},
   "source": [
    "### Hyperparameter Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "204e9f9d-1564-49a4-9832-9e186a29788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparam_study(environment, steps, seed, flip_mode):\n",
    "    combinations_dict_list = []\n",
    "    # Sweep temperature\n",
    "    for temperature in [1*10**-x for x in range(0,5)]:\n",
    "    # Sweep actor step size\n",
    "        for actor_step_size in [1*10**-x for x in range(0,5)]:\n",
    "    # Sweep critic step size\n",
    "            for critic_step_size in [1*10**-x for x in range(0,5)]:\n",
    "    # sweep avg reward step size\n",
    "                for avg_reward_step_size in [1*10**-x for x in range(0,5)]:\n",
    "                    combinations_dict_list.append(\n",
    "                        {'temperature': temperature,\n",
    "                         'actor_step_size': actor_step_size,\n",
    "                         'critic_step_size': critic_step_size,\n",
    "                         'avg_reward_step_size': avg_reward_step_size\n",
    "                        }\n",
    "                    )\n",
    "    results_dict_list = []\n",
    "    for combination in tqdm(combinations_dict_list):\n",
    "        total_reward = run_agent_experiment(environment, steps, seed, flip_mode, **combination)\n",
    "        combination['total_reward'] = total_reward\n",
    "        results_dict_list.append(combination)\n",
    "    return pd.DataFrame(results_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856f0d3-f06c-4950-a3b4-a6340dd5e106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
